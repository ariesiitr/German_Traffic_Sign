{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4454007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "os.chdir('C:/Users/acer/Desktop/Aries/archive')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a554e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "# We have 43 Classes\n",
    "classes = 43\n",
    "cur_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4a60537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\acer\\\\Desktop\\\\Aries\\\\archive'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8215398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "for i in range(classes):\n",
    "    print(i)\n",
    "    path = os.path.join(cur_path,'train',str(i))\n",
    "    images = os.listdir(path)\n",
    "    for a in images:\n",
    "        try:\n",
    "            image = Image.open(path + '\\\\'+ a)\n",
    "            image = image.resize((30,30))\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d0b8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd4099c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('training')\n",
    "np.save('./training/data',data)\n",
    "np.save('./training/target',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c597a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('./training/data.npy')\n",
    "labels=np.load('./training/target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1efaec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 30, 30, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d31f5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "effefbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31367, 30, 30, 3) (7842, 30, 30, 3) (31367,) (7842,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5932082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 43)\n",
    "y_test = to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b390b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "# We have 43 classes that's why we have defined 43 in the dense\n",
    "model.add(Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dfff1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cfbc77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "981/981 [==============================] - 64s 63ms/step - loss: 1.8000 - accuracy: 0.5573 - val_loss: 0.3111 - val_accuracy: 0.9280\n",
      "Epoch 2/20\n",
      "981/981 [==============================] - 57s 58ms/step - loss: 0.4795 - accuracy: 0.8711 - val_loss: 0.1856 - val_accuracy: 0.9494\n",
      "Epoch 3/20\n",
      "981/981 [==============================] - 69s 70ms/step - loss: 0.3096 - accuracy: 0.9173 - val_loss: 0.0936 - val_accuracy: 0.9735\n",
      "Epoch 4/20\n",
      "981/981 [==============================] - 69s 70ms/step - loss: 0.2372 - accuracy: 0.9345 - val_loss: 0.0737 - val_accuracy: 0.9776\n",
      "Epoch 5/20\n",
      "981/981 [==============================] - 69s 70ms/step - loss: 0.2299 - accuracy: 0.9380 - val_loss: 0.0580 - val_accuracy: 0.9844\n",
      "Epoch 6/20\n",
      "981/981 [==============================] - 67s 69ms/step - loss: 0.2201 - accuracy: 0.9406 - val_loss: 0.0508 - val_accuracy: 0.9878\n",
      "Epoch 7/20\n",
      "981/981 [==============================] - 66s 67ms/step - loss: 0.1996 - accuracy: 0.9462 - val_loss: 0.0599 - val_accuracy: 0.9844\n",
      "Epoch 8/20\n",
      "981/981 [==============================] - 64s 66ms/step - loss: 0.2132 - accuracy: 0.9450 - val_loss: 0.0631 - val_accuracy: 0.9836\n",
      "Epoch 9/20\n",
      "981/981 [==============================] - 63s 64ms/step - loss: 0.1972 - accuracy: 0.9511 - val_loss: 0.0825 - val_accuracy: 0.9754\n",
      "Epoch 10/20\n",
      "981/981 [==============================] - 64s 65ms/step - loss: 0.1803 - accuracy: 0.9541 - val_loss: 0.0745 - val_accuracy: 0.9820\n",
      "Epoch 11/20\n",
      "981/981 [==============================] - 71s 73ms/step - loss: 0.1778 - accuracy: 0.9554 - val_loss: 0.0413 - val_accuracy: 0.9890\n",
      "Epoch 12/20\n",
      "981/981 [==============================] - 72s 74ms/step - loss: 0.2072 - accuracy: 0.9508 - val_loss: 0.0647 - val_accuracy: 0.9816\n",
      "Epoch 13/20\n",
      "981/981 [==============================] - 69s 70ms/step - loss: 0.1849 - accuracy: 0.9548 - val_loss: 0.1221 - val_accuracy: 0.9697\n",
      "Epoch 14/20\n",
      "981/981 [==============================] - 66s 68ms/step - loss: 0.1709 - accuracy: 0.9599 - val_loss: 0.1249 - val_accuracy: 0.9642\n",
      "Epoch 15/20\n",
      "981/981 [==============================] - 72s 74ms/step - loss: 0.1819 - accuracy: 0.9572 - val_loss: 0.0624 - val_accuracy: 0.9865\n",
      "Epoch 16/20\n",
      "981/981 [==============================] - 66s 68ms/step - loss: 0.1897 - accuracy: 0.9569 - val_loss: 0.0639 - val_accuracy: 0.9858\n",
      "Epoch 17/20\n",
      "981/981 [==============================] - 94s 96ms/step - loss: 0.1923 - accuracy: 0.9574 - val_loss: 0.0767 - val_accuracy: 0.9846\n",
      "Epoch 18/20\n",
      "981/981 [==============================] - 97s 99ms/step - loss: 0.1824 - accuracy: 0.9596 - val_loss: 0.0382 - val_accuracy: 0.9909\n",
      "Epoch 19/20\n",
      "981/981 [==============================] - 96s 98ms/step - loss: 0.2099 - accuracy: 0.9551 - val_loss: 0.0503 - val_accuracy: 0.9867\n",
      "Epoch 20/20\n",
      "981/981 [==============================] - 101s 103ms/step - loss: 0.1700 - accuracy: 0.9634 - val_loss: 0.0390 - val_accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5d40f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(testcsv):\n",
    "    y_test = pd.read_csv(testcsv)\n",
    "    label = y_test[\"ClassId\"].values\n",
    "    imgs = y_test[\"Path\"].values\n",
    "    data=[]\n",
    "    for img in imgs:\n",
    "        image = Image.open(img)\n",
    "        image = image.resize((30,30))\n",
    "        data.append(np.array(image))\n",
    "    X_test=np.array(data)\n",
    "    return X_test,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d9ab38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, label = testing('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "546bd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 11s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred_prob = model.predict(X_test)\n",
    "Y_pred = np.argmax(Y_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21be378c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  1, 38, ..., 12,  7, 10], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "357848da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665083135391924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(label, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b04b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training/Avnish\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./training/Avnish\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./training/Avnish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "231cc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/acer/Desktop/Aries/archive')\n",
    "from keras.models import load_model\n",
    "model = load_model('./training/Avnish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "233452db",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adc3877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def test_on_img(img):\n",
    "    data=[]\n",
    "    image = Image.open(img)\n",
    "    image = image.resize((30,30))\n",
    "    data.append(np.array(image))\n",
    "    X_test=np.array(data)\n",
    "    Y_pred = model.predict_classes(X_test)\n",
    "    return image,Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbd14fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot,prediction \u001b[38;5;241m=\u001b[39m \u001b[43mtest_on_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/acer/Desktop/Aries/archive/Test/00057.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m s \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m prediction] \n\u001b[0;32m      3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s)) \n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mtest_on_img\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(image))\n\u001b[0;32m      9\u001b[0m X_test\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(data)\n\u001b[1;32m---> 10\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(X_test)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image,Y_pred\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "plot,prediction = test_on_img(\"C:/Users/acer/Desktop/Aries/archive/Test/00057.png\")\n",
    "s = [str(i) for i in prediction] \n",
    "a = int(\"\".join(s)) \n",
    "print(\"Predicted traffic sign is: \", classes[a])\n",
    "plt.imshow(plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b3c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
